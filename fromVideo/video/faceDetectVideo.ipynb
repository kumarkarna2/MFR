{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m _, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Apply a slight blur to reduce noise\u001b[39;00m\n\u001b[0;32m     17\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmedianBlur(gray, \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade specifically trained for low resolution\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\")\n",
    "\n",
    "# To capture video from existing video.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a slight blur to reduce noise\n",
    "    gray = cv2.medianBlur(gray, 5)\n",
    "\n",
    "    # Detect faces at a lower scale factor\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.05, 3)\n",
    "\n",
    "    # Draw the rectangle around each face\n",
    "    for x, y, w, h in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
    "\n",
    "    # Display\n",
    "    cv2.imshow(\"Face Window\", img)\n",
    "\n",
    "    #stop if length of video is reached\n",
    "    if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    # Stop if escape key is pressed\n",
    "    k = cv2.waitKey(30) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "\n",
    "# Release the VideoCapture object\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "\n",
    "def load_known_faces(known_faces_directory):\n",
    "    known_faces = []\n",
    "    known_face_encodings = []\n",
    "\n",
    "    for root, _, files in os.walk(known_faces_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                full_path = os.path.join(root, filename)\n",
    "                known_face = cv2.imread(full_path)\n",
    "                bgr_to_rgb = cv2.cvtColor(known_face, cv2.COLOR_BGR2RGB)\n",
    "                face_encoding = face_recognition.face_encodings(bgr_to_rgb)[0]\n",
    "\n",
    "                known_faces.append(known_face)\n",
    "                known_face_encodings.append(face_encoding)\n",
    "\n",
    "    return known_faces, known_face_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the known faces and their encodings\n",
    "known_faces_directory = r\"M:/Database/karna/\"\n",
    "known_faces, known_face_encodings = load_known_faces(known_faces_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video_frame(img):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply contrast enhancement using CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(2, (8, 8))\n",
    "    gray = clahe.apply(gray)\n",
    "\n",
    "    # Apply noise reduction using bilateral filtering\n",
    "    gray = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "\n",
    "    # Convert the grayscale image back to RGB format\n",
    "    img_preprocessed = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return img_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m img_preprocessed \u001b[38;5;241m=\u001b[39mimg\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Apply a slight blur to reduce noise\u001b[39;00m\n\u001b[0;32m     25\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmedianBlur(gray, \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "# Load the cascade specifically trained for low resolution\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\")\n",
    "\n",
    "# Load the known faces and their encodings\n",
    "known_faces, known_face_encodings = load_known_faces(\"known_faces\")\n",
    "\n",
    "# To capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "\n",
    "    # Preprocess the video frame to enhance face detection\n",
    "    # img_preprocessed = preprocess_video_frame(img)\n",
    "    img_preprocessed =img\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img_preprocessed, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a slight blur to reduce noise\n",
    "    gray = cv2.medianBlur(gray, 5)\n",
    "\n",
    "    # Detect faces at a lower scale factor\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.05, 3)\n",
    "\n",
    "    # For each face\n",
    "    for x, y, w, h in faces:\n",
    "        # Extract the face region\n",
    "        face = img_preprocessed[y : y + h, x : x + w]\n",
    "\n",
    "        # Convert the face region to RGB format\n",
    "        face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Find the face encodings in the current frame\n",
    "        face_encodings = face_recognition.face_encodings(face_rgb)\n",
    "\n",
    "        # Check if there are any faces in the current frame\n",
    "        if len(face_encodings) > 0:\n",
    "            # Compare the face encodings in the current frame to the known faces\n",
    "            matches = face_recognition.compare_faces(\n",
    "                known_face_encodings, face_encodings[0]\n",
    "            )\n",
    "\n",
    "            # If a match is found, identify the person\n",
    "            if any(match for match in matches):\n",
    "                index = matches.index(True)\n",
    "                known_face = known_faces[index]\n",
    "\n",
    "                # Draw a rectangle around the face and display the person's name\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(\n",
    "                    img,\n",
    "                    known_face,\n",
    "                    (x, y - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    (255, 255, 255),\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Video\", img)\n",
    "\n",
    "    # Stop if the escape key is pressed\n",
    "    k = cv2.waitKey(30) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# --- Initialization ---\u001b[39;00m\n\u001b[0;32m     48\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(haar_cascade_file)\n\u001b[1;32m---> 49\u001b[0m known_faces, known_face_encodings \u001b[38;5;241m=\u001b[39m load_known_faces(known_faces_directory)\n\u001b[0;32m     50\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)  \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# --- Main Processing Loop ---\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "known_faces_directory = r\"M:/Database/karna/\"\n",
    "haar_cascade_file = \"haarcascade_frontalface_alt2.xml\"  \n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_known_faces(known_faces_directory):\n",
    "    known_faces = []\n",
    "    known_face_encodings = []\n",
    "\n",
    "    for root, _, files in os.walk(known_faces_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                full_path = os.path.join(root, filename)\n",
    "                known_face = cv2.imread(full_path)\n",
    "                bgr_to_rgb = cv2.cvtColor(known_face, cv2.COLOR_BGR2RGB)\n",
    "                face_encoding = face_recognition.face_encodings(bgr_to_rgb)[0]\n",
    "\n",
    "                known_faces.append(known_face)\n",
    "                known_face_encodings.append(face_encoding)\n",
    "\n",
    "    return known_faces, known_face_encodings\n",
    "\n",
    "\n",
    "def preprocess_video_frame(img):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply contrast enhancement using CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(2, (8, 8))\n",
    "    gray = clahe.apply(gray)\n",
    "\n",
    "    # Apply noise reduction using bilateral filtering\n",
    "    gray = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "\n",
    "    # Convert the grayscale image back to RGB format\n",
    "    img_preprocessed = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return img_preprocessed\n",
    "\n",
    "\n",
    "# --- Initialization ---\n",
    "face_cascade = cv2.CascadeClassifier(haar_cascade_file)\n",
    "known_faces, known_face_encodings = load_known_faces(known_faces_directory)\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "while True:\n",
    "    _, img = cap.read()\n",
    "\n",
    "    # Preprocess\n",
    "    img_preprocessed = preprocess_video_frame(img) \n",
    "\n",
    "    # Grayscale + Blur for Haar Cascade\n",
    "    gray = cv2.cvtColor(img_preprocessed, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.medianBlur(gray, 5) \n",
    "\n",
    "    # Detect with Haar\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.05, 3)\n",
    "\n",
    "    # Face Recognition for Each Detected Face\n",
    "    for x, y, w, h in faces:\n",
    "        face = img_preprocessed[y : y + h, x : x + w] \n",
    "        face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        face_encodings = face_recognition.face_encodings(face_rgb)\n",
    "\n",
    "        if len(face_encodings) > 0:\n",
    "            matches = face_recognition.compare_faces(\n",
    "                known_face_encodings, face_encodings[0]\n",
    "            )\n",
    "\n",
    "            if any(match for match in matches):\n",
    "                index = matches.index(True)\n",
    "                known_person_image = known_faces[index]\n",
    "\n",
    "                # Draw rectangle in green (0, 255, 0)\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2) \n",
    "        \n",
    "                # Draw name below the face\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                name = os.path.basename(known_person_image)\n",
    "                cv2.putText(img, name, (x, y + h + 20), font, 0.7, (255, 255, 255), 1)\n",
    "\n",
    "    # Display output video \n",
    "    cv2.imshow(\"Facial Recognition\", img)\n",
    "\n",
    "    # Exit on 'ESC' key press\n",
    "    k = cv2.waitKey(30) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
